import os
import gc
import sys
import math
import copy
import logging
import numpy as np

import torch
import torch.nn.functional as F
import torch.distributed as dist
from torch.utils.data import DataLoader, Sampler
from torch.utils.data.distributed import DistributedSampler
from transformers import AdamW

from federatedscope.register import register_trainer
# from federatedscope.llm.trainer.trainer_arxivarxiv import LLMTrainer
from federatedscope.llm.trainer.trainer import LLMTrainer
from federatedscope.core.trainers.context import CtxVar, lifecycle
from federatedscope.core.trainers.enums import MODE, LIFECYCLE
from federatedscope.core.auxiliaries.decorators import use_diff
from federatedscope.core.data.wrap_dataset import WrapDataset
from federatedscope.core.auxiliaries.dataloader_builder import get_dataloader
from federatedscope.core.auxiliaries.ReIterator import ReIterator
from federatedscope.core.auxiliaries.optimizer_builder import get_optimizer
from federatedscope.core.auxiliaries.scheduler_builder import get_scheduler
from federatedscope.core.monitors.monitor import Monitor

from federatedscope.llm.dataset.llm_dataset import DefaultToken
from federatedscope.llm.model.adapter_builder import AdapterModel

from torch.optim.lr_scheduler import MultiStepLR

logger = logging.getLogger(__name__)
sys.setrecursionlimit(100000)


def _get_dist_info():
    if dist.is_available() and dist.is_initialized():
        return True, dist.get_world_size(), dist.get_rank()
    try:
        ws = int(os.environ.get('WORLD_SIZE', '1'))
        rk = int(os.environ.get('RANK', '0'))
        return (ws > 1), ws, rk
    except Exception:
        return False, 1, 0

class EvalShardSampler(Sampler):
    def __init__(self, dataset_len: int, rank: int, world_size: int):
        self.indices = list(range(rank, int(dataset_len), int(world_size)))
    def __iter__(self): return iter(self.indices)
    def __len__(self): return len(self.indices)
    
def cal_loss(logits, labels, choices):

    """
    input_ids, labelsê°€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì–´ì¡Œë‹¤ ê°€ì •.
    [<BOS>, â–Question,  :,  â–2+2,  ?,  â–A,  <EOS>]   (T = 7)

    logits

    [â–Question,  :,  â–2+2,  ?,  â–A,  <EOS>, -100]
    
    """


    # 1) next-token ì…‹ì—…: (t-1, t) ì •ë ¬
    choices = choices.to(logits.device) 
    shift_logits = logits[..., :-1, :].contiguous() # [B, T-1, V]. EOS ì— ëŒ€í•œ ì˜ˆì¸¡í•˜ëŠ” POSITIONë§Œ ì œì™¸. Vì¤‘ Max [â–Question,  :,  â–2+2,  ?,  â–A,  <EOS>]
    shift_labels = labels[..., 1:].contiguous() # [B, T-1]. BOS ì— ëŒ€í•œ ì˜ˆì¸¡í•˜ëŠ” POSITIONë§Œ ì œì™¸. [â–Question,  :,  â–2+2,  ?,  â–A,  <EOS>]


    # 2) ë¼ë²¨ ë§µí•‘: A/B í† í°ì´ ë“±ì¥í•œ ìë¦¬ë§Œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤(0/1)ë¡œ ë°”ê¾¸ê³  ë‚˜ë¨¸ì§€ëŠ” IGN(-100
    new_labels = torch.full_like(shift_labels, DefaultToken.IGNORE_INDEX.value) #[-100, -100, -100, -100, -100, -100]ìœ¼ë¡œ ì´ˆê¸°í™”
    for idx, choice in enumerate(choices): # ì˜ˆ: [ID('â–A'), ID('â–B')]
        mask = (shift_labels == choice); new_labels[mask] = idx
    #mask->[False, False, False, False,  True, False]
    #new_labels = [-100, -100, -100, -100, 0, -100]


    # ì˜ˆ: [ID('â–A'), ID('â–B')] ê´€í•œ ê±¸ë¡œë§Œ logit space ì¶•ì†Œ.
    new_logits = shift_logits[..., choices]
    loss_fn = torch.nn.CrossEntropyLoss() #torch.nn.CrossEntropyLoss()ì˜ ê¸°ë³¸ ignore_indexê°€ -100ì´ë¼ì„œ, íƒ€ê¹ƒì— -100ì´ ë“¤ì–´ ìˆìœ¼ë©´ ê·¸ ìœ„ì¹˜ë“¤ì€ ì†ì‹¤/ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ì—ì„œ ì œì™¸ë¼ìš”. í‰ê· ë„ ìœ íš¨í•œ í•­ë§Œìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

    # 4) (BÃ—(T-1), K) vs (BÃ—(T-1))ë¡œ í¼ì³ CrossEntropy
    loss = loss_fn(new_logits.view(-1, len(choices)), new_labels.view(-1)) #ì‹¤ì§ˆì ìœ¼ë¡œ t=4ì¸ ì§€ì ì— ëŒ€í•´ì„œë§Œ ce ê³„ì‚°ë¨.
    return new_logits, new_labels, loss

class RewardChoiceTrainer(LLMTrainer): #LLMì´ ë±‰ëŠ” logits[B, T, V](V=ì „ì²´ vocab)ì— ëŒ€í•´ â€œë‹¤ìŒ í† í°ì´ Aëƒ Bëƒë§Œâ€ ë³´ë„ë¡ ì¶•ì†Œí•´ì„œ **CrossEntropyLoss(BÃ—(T-1) vs 2)**ë¡œ í•™ìŠµ/í‰ê°€í•œë‹¤. 
    def __init__(self, model, data, device, config, only_for_eval=False, monitor=None):
        super().__init__(model, data, device, config, only_for_eval, monitor)

        #ì‹¤ì œ ë¼ë²¨ì€ " A</s>"= "â–A"+"</s>" or " B</s>"="â–B"+"</s>"ì¸ ìƒí™©. ìµœì¢… ëª©í‘œ: "â–A", "â–B" ì´ë ‡ê²Œ 2ì°¨ì›ìœ¼ë¡œ Classifierë¥¼ ì¶•ì†Œ.
        try:
            # (ì¤‘ìš”) "choices" ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬
            # ë§ˆì§€ë§‰ í† í° IDë§Œ ì¶”ì¶œ â†’ í•´ë‹¹ í† í° ë“±ì¥ ì‹œ ê·¸ ìœ„ì¹˜ë¥¼ ì„ íƒì§€ í´ë˜ìŠ¤ ë¼ë²¨ë¡œ ì‚¬ìš©
            choices_list = [] #"â–A", "â–B"ì˜ token id ì¶”ì¶œì´ ëª©í‘œ.
            for choice in config.trainer.choices: #config.trainer.choices=["A", "B"]
                # ì•ì— ': 'ë¥¼ ë¶™ì´ëŠ” ì´ìœ : í”„ë¡¬í”„íŠ¸ í˜•ì‹ì—ì„œ ì‘ë‹µ í‘œê¸° ì§í›„ í† í°ì„ ì•ˆì •ì ìœ¼ë¡œ ë½‘ê¸° ìœ„í•¨. add_special_tokens=False:BOS/EOS ê°™ì€ ìŠ¤í˜ì…œ í† í°ì´ ë¼ì–´ë“¤ë©´ ì•ˆ ë˜ë¯€ë¡œ.
                token_ids = self.tokenizer(f': {choice}', add_special_tokens=False)['input_ids']
                #": A"ëŠ” ë³´í†µ [:, â–A]ì²˜ëŸ¼ 2ê°œ ì´ìƒ í† í°ìœ¼ë¡œ ìª¼ê°œì§‘ë‹ˆë‹¤. ì´ ì¤‘ì— **ì‹¤ì œë¡œ ë¶„ë¥˜ë¥¼ ê°ˆë¼ì£¼ëŠ” ê±´ ë§ˆì§€ë§‰ í† í°(= â–A)**ì´ì£ . ê·¸ë˜ì„œ [-1]ë§Œ ë½‘ìŠµë‹ˆë‹¤.
                if not token_ids: raise ValueError(f"Tokenizer returned empty list for choice: '{choice}'")
                choices_list.append(token_ids[-1])
            # CPU í…ì„œë¡œ ë³´ê´€, ë¼ìš´ë“œ ì‹œì‘ ì‹œ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¹€
            self.choices_cpu = torch.tensor(choices_list, dtype=torch.long)
            logger.info(f'Choice token IDs: {self.choices_cpu.tolist()}')
        except Exception as e:
            logger.error(f"Error during trainer initialization: {e}")
            raise ValueError('Failed to initialize trainer.choices.')
        
    def _reset_and_build_dataloader(self, split_name): #í•™ìŠµ/í‰ê°€ì— ë“¤ì–´ê°€ê¸° ì§ì „ë§ˆë‹¤ ë¡œë”ë¥¼ ë‹¤ì‹œ ë§Œë“¤ê³ (ìƒ¤ë”© ë°˜ì˜)
        data_key = f"{split_name}_data"
        loader_key = split_name
        ctx_loader_key = f"{split_name}_loader"

        client_data_obj = self.ctx.data

        # ê¸°ì¡´ ë¡œë” ì œê±°. ê°™ì€ ë¼ìš´ë“œ/ë£¨í‹´ ë°˜ë³µ ì‹œ ë‚¡ì€ ì´í„°ë ˆì´í„°/ìƒ˜í”ŒëŸ¬ ìƒíƒœê°€ ì„ì´ì§€ ì•Šë„ë¡ ê¸°ì¡´ ë¡œë”ë¥¼ ê¹¨ë—ì´ ì§€ì›€.
        if loader_key in client_data_obj:
            del client_data_obj[loader_key]

        # ì›ë³¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë” ìƒì„±
        if hasattr(client_data_obj, data_key) and getattr(client_data_obj, data_key) is not None:
            dataset = WrapDataset(getattr(client_data_obj, data_key))
            base_loader = get_dataloader(dataset, self.cfg, split_name) #sharding ì´ì „ì˜ ìƒí™©. ìƒ¤ë”© ì „(sampler=ì—†ê±°ë‚˜ ê¸°ë³¸).


            #dist_ready: torch.distributed í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€(bool)

            #world_size: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜(ì˜ˆ: 4)

            #rank: í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ì „ì—­ ë­í¬(0,1,2,3 â€¦)

            dist_ready, world_size, rank = _get_dist_info()


            if world_size > 1:
                if split_name == "train":

                   # DistributedSampler (í›ˆë ¨ìš©)
                    #### ëª©ì : ê° ì—í­ë§ˆë‹¤ ì…”í”Œí•˜ê³ , ê° rankì— ë™ì¼ ê°œìˆ˜ì˜ ìƒ˜í”Œì„ ë°°ë¶„.

                    #### ì‘ë™:
                    ######## ì—í­ë§ˆë‹¤ set_epoch(e)ë¡œ ì‹œë“œ ê³ ì • â†’ ì „ rankê°€ ë™ì¼í•œ í¼ë®¤í…Œì´ì…˜ì„ ê³µìœ .

                    ######## ì „ì²´ ì¸ë±ìŠ¤ë¥¼ ì„ì€ ë’¤ ë™ì¼í•œ ê¸¸ì´ë¡œ ê· ë“± ë¶„í• .

                    ######## drop_last=Falseë©´ ê¸¸ì´ê°€ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œ ì•ìª½ ì¸ë±ìŠ¤ë¥¼ ì¬ì‚¬ìš©(íŒ¨ë”©) í•´ì„œ num_samples = ceil(N / world_size)ë¥¼ ë§ì¶¤ â†’ í•œ ì—í­ ë‚´ ì¤‘ë³µ ê°€ëŠ¥(í›ˆë ¨ì—ì„œëŠ” í—ˆìš©/ê¶Œì¥).

                    #### ê¸¸ì´:

                    ######## len(sampler) = num_samples = ceil(N / world_size) (drop_last=False)

                    ######## íŠ¹ì§•: ì…”í”Œ/íŒ¨ë”©/ê· ë“±ë¶„í• /set_epoch ì§€ì› â†’ í›ˆë ¨ ì¹œí™”ì .


                    # num_samples = ceil(10/4)=3, total_size=12 â†’ ì•ì—ì„œ 2ê°œ íŒ¨ë”©(ì¤‘ë³µ)

                    # rank0: [0,1,2]
                    # rank1: [3,4,5]
                    # rank2: [6,7,8]
                    # rank3: [9,0,1] âŸµ íŒ¨ë”©ëœ ì¤‘ë³µ

                    # ì¥ì : ê° rank 3ê°œë¡œ ê· ë“± / ë‹¨ì : ì¤‘ë³µ ì¡´ì¬(í›ˆë ¨ OK)

                    sampler = DistributedSampler(
                        base_loader.dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=True,
                    )
                    self._train_dist_sampler = sampler

                    loader = DataLoader(
                        dataset=base_loader.dataset,
                        batch_size=base_loader.batch_size,
                        sampler=sampler,
                        shuffle=False,
                        num_workers=getattr(base_loader, "num_workers", 0),
                        pin_memory=getattr(base_loader, "pin_memory", False),
                        drop_last=getattr(base_loader, "drop_last", False),
                        collate_fn=getattr(base_loader, "collate_fn", None),
                    )
                else:
                    # EvalShardSampler

                    # rank0: [0,4,8] (3ê°œ)
                    # rank1: [1,5,9] (3ê°œ)
                    # rank2: [2,6] (2ê°œ)
                    # rank3: [3,7] (2ê°œ)

                    # ì¥ì : ì¤‘ë³µ ì—†ìŒ, ì „ì²´ ì •í™•íˆ 10ê°œ í‰ê°€ / ë‹¨ì : ê° rank ê¸¸ì´ê°€ ê· ë“±í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ(í‰ê°€ OK)

                    sampler = EvalShardSampler(len(base_loader.dataset), rank, world_size)

                    loader = DataLoader(
                        dataset=base_loader.dataset,
                        batch_size=base_loader.batch_size,
                        sampler=sampler,
                        shuffle=False,
                        num_workers=0,
                        pin_memory=False,
                        drop_last=False,
                        collate_fn=getattr(base_loader, "collate_fn", None),
                    )

                sharded = True
                local_count = len(sampler)
            else:
                loader = base_loader
                sharded = False
                local_count = len(base_loader.dataset)

            client_data_obj[loader_key] = loader
            setattr(self.ctx, ctx_loader_key, loader)

            logger.info(
                f"Dataloader for '{split_name}' has been reset and recreated. "
                f"(sharded={sharded}, "
                f"world_size={1 if not sharded else world_size}, "
                f"rank={0 if not sharded else rank}, "
                f"local_count={local_count}, "
                f"total={len(base_loader.dataset)})"
            )



    @use_diff
    def train(self, target_data_split_name="train", hooks_set=None, round_num=-1):
        # [ìˆ˜ì •] current_roundë¥¼ ë§¨ ë¨¼ì € ì •ì˜í•˜ì—¬ NameError í•´ê²°
        current_round = round_num
        self.ctx.current_round_num = current_round

        scheduler_cfg = getattr(self.cfg.train, "scheduler", None)
        if scheduler_cfg:
            initial_lr = self.cfg.train.optimizer.lr
            milestones = getattr(scheduler_cfg, "milestones", [])
            gamma = getattr(scheduler_cfg, "gamma", 1.0)
            
            num_decays = sum(1 for milestone in sorted(milestones) if current_round >= milestone)
            new_lr = initial_lr * (gamma ** num_decays)
            
            self.ctx.new_lr_to_be_set = new_lr
            logger.info(
                f"[Stateless LR Controller] In Round #{current_round}, planning to set LR to {new_lr:.2e}"
            )

        self._reset_and_build_dataloader(target_data_split_name)
        return super().train(target_data_split_name, hooks_set)

    def evaluate(self, target_data_split_name="test", hooks_set=None):
        hooks_set = hooks_set or self.hooks_in_eval
        self._reset_and_build_dataloader(target_data_split_name)
        with torch.no_grad():
            if self.ctx.check_split(target_data_split_name, skip=True):
                self._run_routine(MODE.TEST, hooks_set, target_data_split_name)
            else:
                self.ctx.eval_metrics = dict()
        return self.ctx.eval_metrics

    def _hook_on_fit_start_init(self, ctx):
        super()._hook_on_fit_start_init(ctx)
        
        if hasattr(ctx, "new_lr_to_be_set"):
            new_lr = ctx.new_lr_to_be_set
            if hasattr(ctx, 'optimizer') and ctx.optimizer is not None:
                opt = ctx.optimizer
                for param_group in opt.param_groups:
                    param_group['lr'] = new_lr
                logger.info(f"Successfully applied new LR {new_lr:.2e} to the optimizer.")
                del ctx.new_lr_to_be_set
        
        current_round = int(getattr(ctx, "current_round_num", getattr(ctx, "cur_state", 0)))
        self.choices = self.choices_cpu.to(ctx.device, non_blocking=True) #non_blocking=TrueëŠ” pinned memory í™˜ê²½ì—ì„œ async copy í—ˆìš© â†’ ì„±ëŠ¥ ìµœì í™”.
 

        # train/val/test ì„±ëŠ¥ ì¸¡ì •ìš© ì¹´ìš´í„° ë¦¬ì…‹
        for sp in ["train", "val", "test"]:
            setattr(ctx, f"num_samples_{sp}", 0)
            setattr(ctx, f"loss_total_{sp}", 0.0)
            setattr(ctx, f"correct_{sp}", 0)

        #ë¼ìš´ë“œ ì „ì²´ ì¹´ìš´í„° ë¦¬ì…‹

        ctx.num_samples = 0
        ctx.loss_batch_total = 0.0
        ctx.loss_regular_total = 0.0


        ctx.sample_seen = 0
        ctx.sample_correct_accum = 0


    def _hook_on_batch_forward(self, ctx): #ì°¸ê³ 

        #LLM í•™ìŠµìš© Datasetì€ ë³´í†µ tokenized_dictë¥¼ ë°˜í™˜->{'input_ids': Tensor, 'labels': Tensor, 'attention_mask': Tensor}
        #acceleratorëŠ” ì…ë ¥ í…ì„œë¥¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ì— ì˜¬ë ¤ì¤˜ì„œ .to(ctx.device)ê°€ í•„ìš” ì—†ìŒ.


        #input_ids: [101,  42,  53,  78,   2]
        #labels:    [101,  42,  53,  78,   2]  # í˜¹ì€ ì¼ë¶€ -100 í¬í•¨
        #ì´ë ‡ê²Œ ê±°ì˜ ê°™ì§€ë§Œ, loss ê³„ì‚°ì—ì„œ ë¬´ì‹œí•  í† í°ì€ labelsì—ì„œë§Œ ë°”ë€œ.

        # 1) ì…ë ¥ ì¤€ë¹„
        input_ids = ctx.data_batch["input_ids"].to(ctx.device)
        labels = ctx.data_batch["labels"].to(ctx.device)
        attention_mask = ctx.data_batch["attention_mask"].to(ctx.device)

        # 2) ëª¨ë¸ ì‹¤í–‰ 
        if ctx.cfg.llm.accelerator.use: #ì°¸ê³ , #ctx.model ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )
        elif ctx.cfg.llm.deepspeed.use: #ctx.model_engine ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model_engine(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )
        else: #ì°¸ê³ , #ctx.model ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )


        # 3) ì»¤ìŠ¤í…€ loss ê³„ì‚° 
        logits = outputs.logits #ì¼ë°˜ì ìœ¼ë¡œ LLMì—ì„œëŠ” [batch_size, seq_len, vocab_size] shapeì˜ tensor



        ################         ì´ ë¶€ë¶„ì´ ë‹¬ë¼ì§       #######################
        #new_labels = [-100, -100, -100, -100, 0, -100]
        new_logits, new_labels, loss = cal_loss(logits, labels, self.choices)

        # 4) NaN ë°©ì–´ ë¡œì§
        if torch.isnan(loss): #LM í•™ìŠµì—ì„œëŠ” ì¢…ì¢… NaN lossê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ-> labelì´ ëª¨ë‘ -100 (ignore index). precision ë¬¸ì œ (e.g., bf16/float16). exploding gradients, bad initialization
            ctx.skip_this_batch = CtxVar(True, LIFECYCLE.BATCH) #ë‹¤ë¥¸ hookì—ì„œ ì´ ê°’ì´ Trueë©´ ì´ ë°°ì¹˜ë¥¼ ê±´ë„ˆëœ€. (ì˜ˆ: loss.backward() ìŠ¤í‚µ)
            logger.warning("Skip the batch due to the loss is NaN, '
                           'it may be caused by exceeding the precision or '
                           'invalid labels.')
            return
        else:
            ctx.skip_this_batch = CtxVar(False, LIFECYCLE.BATCH)

        # 5) ì²« ìœ íš¨ í† í° ì •í™•ë„
        #LLM í•™ìŠµì—ì„œëŠ” labelsì— -100(IGNORE_INDEX)ì´ ì„ì—¬ ìˆìŒ â†’ loss/accì— í¬í•¨ë˜ì§€ ì•ŠìŒ.
        #ì—¬ê¸°ì„œëŠ” ë°°ì¹˜ë§ˆë‹¤ â€œì²« ë²ˆì§¸ ìœ íš¨ í† í°â€ë§Œ ë½‘ì•„ì„œ accuracyë¥¼ ê³„ì‚°.
        #new_labels = [-100, -100, -100, -100, 0, -100]

        IGN = DefaultToken.IGNORE_INDEX.value # ë³´í†µ -100
        valid_mask = (new_labels != IGN) # ìœ íš¨ í† í° ìœ„ì¹˜ë§Œ True
        has_valid = valid_mask.any(dim=1) # ê° ìƒ˜í”Œì— ìœ íš¨ í† í°ì´ ìˆëŠ”ì§€ ì—¬ë¶€

        B = new_labels.size(0)
        arangeB = torch.arange(B, device=new_labels.device) #[0,1, ..., B-1]
        first_idx = torch.argmax(valid_mask.int(), dim=1) #ìƒ˜í”Œë³„ ì²˜ìŒìœ¼ë¡œ ìœ íš¨ í† í°ì— ë“±ì¥í•˜ëŠ” ìœ„ì¹˜

        sel_b = arangeB[has_valid] #ìœ íš¨ í† í° ìˆëŠ” Indexë“¤ë§Œ
        sel_t = first_idx[has_valid] #ìœ íš¨ í† í° ìˆëŠ” ìƒ˜í”„ë“¤ ê¸°ì¤€ ì²« ìœ íš¨ í† í° ìœ„ì¹˜ ì§‘í•©.

        if sel_b.numel() > 0:
            logits_1st = new_logits[sel_b, sel_t, :] #new logit ì¤‘ ìœ íš¨ í† í° ìˆëŠ” ê²ƒë“¤ í•œí•´ì„œ ì „ì²´ vocab ì¤‘ classifierì— í•´ë‹¹í•˜ëŠ” ì¶•ì†Œëœ logit ë°˜í™˜. ì¦‰  (|sel_b|, 1, C) ì°¨ì›.
            labels_1st = new_labels[sel_b, sel_t] #|sel_b|, 1 Classification true labelë§Œ ë½‘ìŒ.
            pred_1st = torch.argmax(logits_1st, dim=-1) #|sel_b| ê°œì— ëŒ€í•´ ì˜ˆì¸¡ label ë½‘ìŒ.

            sample_correct = int((pred_1st == labels_1st).sum().item()) #ë§ì¶˜ ê°¯ìˆ˜
            sample_count = int(has_valid.sum().item()) #ì „ì²´ sample ê°¯ìˆ˜.
        else:
            sample_correct, sample_count = 0, 0



        ctx.sample_correct_batch = sample_correct #ë°°ì¹˜ ë‚´ì—ì„œ ìœ íš¨í•œ ê²ƒ ì¤‘ ë§ì¶˜ ê²ƒ ê°¯ìˆ˜
        ctx.sample_count_batch = sample_count #ë°°ì¹˜ ë‚´ì˜ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜.



        # í‰íƒ„í™” í›„ ì „ì²´ í† í° ë‹¨ìœ„ ì˜ˆì¸¡/ì •ë‹µ ì €ì¥
        flat_labels = new_labels.view(-1) #[B*(T-1)]
        flat_logits = new_logits.view(-1, new_logits.size(-1)) #[B*(T-1), V]



        keep = (flat_labels != IGN) #ë¬´ì‹œ í•˜ì§€ë§ì•„ì•¼í•  í† í° ì°¾ê¸°.
        flat_logits = flat_logits[keep, :] #ë¬´ì‹œí•´ì•¼í•  í† í°ë“¤ë§Œ ì œê±° #[N_valid, V]
        flat_labels = flat_labels[keep] #ë¬´ì‹œí•´ì•¼í•  í† í°ë“¤ë§Œ ì œê±° #[N_valid]

        _, predicted = flat_logits.max(1) #[N_valid]



        ctx.y_true = CtxVar(flat_labels, LIFECYCLE.BATCH) #[N_valid]
        ctx.y_pred = CtxVar(predicted, LIFECYCLE.BATCH) #[N_valid]
        ctx.y_prob = CtxVar(flat_logits, LIFECYCLE.BATCH) #[N_valid, V]

        ctx.loss_batch = CtxVar(loss, LIFECYCLE.BATCH) #Scalar.
        ctx.batch_size = CtxVar(len(labels), LIFECYCLE.BATCH) #B. ì¼ë°˜ì ìœ¼ë¡œ B=N_valid.

 

 
    def _hook_on_batch_end(self, ctx): #ğŸ‘‰ í•œ ë°°ì¹˜(batch)ë¥¼ ëë‚¼ ë•Œ í˜¸ì¶œë˜ëŠ” í›„ì²˜ë¦¬ hook
        #1) ìŠ¤í‚µ ì²˜ë¦¬
        skip = getattr(ctx, "skip_this_batch", False)
        if isinstance(skip, CtxVar):
            try:
                skip = bool(skip)
            except Exception:
                skip = False
        if skip:
            return

        #2) (ì˜µì…˜) ë°°ì¹˜ë³„ LR ë””ë²„ê¹…. ìŠ¤ì¼€ì¥´ëŸ¬ ì ìš© í™•ì¸ìš©.
        if ctx.cur_mode == MODE.TRAIN:
            opt = getattr(ctx, "optimizer", None)
            if opt is not None:
                current_lr = opt.param_groups[0]["lr"]
                round_idx = getattr(ctx, "current_round_num", "?")
                batch_idx = getattr(ctx, "batch_idx", "?")
                # logger.info(f"[LR BATCH CHECK] round={round_idx} batch={batch_idx} -> LR={current_lr:.2e}")

        
        #3) í†µê³„ ì§‘ê³„ì‹œ í‚¤ ì´ˆê¸°í™”
        # ëˆ„ì  ì§‘ê³„ í‚¤
        split = ctx.cur_split
        loss_total_key = f"loss_total_{split}"
        correct_key = f"correct_{split}"
        num_samples_key = f"num_samples_{split}"


        if not hasattr(ctx, loss_total_key):
            setattr(ctx, loss_total_key, 0.0)
        if not hasattr(ctx, correct_key):
            setattr(ctx, correct_key, 0)
        if not hasattr(ctx, num_samples_key):
            setattr(ctx, num_samples_key, 0)

        #4) ë°°ì¹˜ ë‹¨ìœ„ í†µê³„
        batch_raw_samples = int(ctx.batch_size) #ë°°ì¹˜ ë‚´ì˜ ì „ì²´ ìƒ˜í”Œ ìˆ˜
        batch_logic_seen = int(getattr(ctx, "sample_count_batch", 0)) #ë°°ì¹˜ ë‚´ì˜ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜
        batch_logic_corr = int(getattr(ctx, "sample_correct_batch", 0)) #ë°°ì¹˜ ë‚´ì˜ ìœ íš¨í•œ ìƒ˜í”Œë“¤ ì¤‘ ë§ì¶˜ ê²ƒë“¤ì˜ ê°¯ìˆ˜

        #split ë™ì•ˆ ëˆ„ì  sample ìˆ˜ ë§Ÿ loss ì´í•©
        setattr(
            ctx,
            loss_total_key,
            getattr(ctx, loss_total_key) + ctx.loss_batch.item() * batch_raw_samples,
        )

        setattr(
            ctx,
            num_samples_key,
            getattr(ctx, num_samples_key) + batch_raw_samples,
        )


        #5) ì •í™•ë„ ëˆ„ì ,  ë…¼ë¦¬ì (ì„ íƒëœ) í† í° ê¸°ì¤€ ì •í™•ë„ ì§‘ê³„
        ctx.sample_seen = int(getattr(ctx, "sample_seen", 0)) + batch_logic_seen
        ctx.sample_correct_accum = int(getattr(ctx, "sample_correct_accum", 0)) + batch_logic_corr

        #6) Loss ì„¸ë¶€ ì§‘ê³„
        ctx.num_samples = int(getattr(ctx, "num_samples", 0)) + batch_raw_samples
        ctx.loss_batch_total = float(getattr(ctx, "loss_batch_total", 0.0)) + ctx.loss_batch.item() * batch_raw_samples
        ctx.loss_regular_total = float(getattr(ctx, "loss_regular_total", 0.0)) + float(ctx.get("loss_regular", 0.0))

        #7) ê²€ì¦/í…ŒìŠ¤íŠ¸: Prediction ì •ë³´ ì €ì¥
        if ctx.cur_mode in [MODE.TEST, MODE.VAL]:
            if not hasattr(ctx, "ys_true"):
                ctx.ys_true = []
            if not hasattr(ctx, "ys_pred"):
                ctx.ys_pred = []

            pred = torch.argmax(ctx.y_prob, dim=-1)
            ctx.ys_true.append(ctx.y_true)
            ctx.ys_pred.append(pred)


    def _ddp_reduce_split(self, ctx, split: str):
        """
        ë©€í‹° GPU / DDP(Accelerate) í™˜ê²½ì—ì„œ íŠ¹ì • split(train/val/test)ì˜
        ìƒ˜í”Œ ìˆ˜ì™€ ì†ì‹¤ ì´í•©ì„ reduce(ì§‘ê³„)í•˜ëŠ” í•¨ìˆ˜.
        """

        # Accelerator ê°ì²´ì™€ í˜„ì¬ ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        acc = getattr(self, "accelerator", None)
        device = ctx.device

        # ë¡œì»¬ ê°’: ìƒ˜í”Œ ìˆ˜ì™€ ì†ì‹¤ í•©ê³„ë¥¼ í…ì„œë¡œ ë³€í™˜
        n = torch.tensor(
            [getattr(ctx, f"num_samples_{split}", 0)],
            device=device,
            dtype=torch.long
        )
        loss_sum = torch.tensor(
            [getattr(ctx, f"loss_total_{split}", 0.0)],
            device=device,
            dtype=torch.float32
        )

        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ë¼ë©´ ëª¨ë“  rank ê°’ í•©ì‚°
        if acc is not None and acc.num_processes > 1:
            n = acc.reduce(n, reduction="sum") #ìƒ˜í”Œ ìˆ˜ ì´í•©
            loss_sum = acc.reduce(loss_sum, reduction="sum") #loss ì´í•©.

        # ìµœì¢… ê²°ê³¼: íŒŒì´ì¬ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜
        return int(n.item()), float(loss_sum.item())

    def _hook_on_fit_end(self, ctx):
        #1) split ìœ íš¨ì„± ì²´í¬
        split = getattr(ctx, "cur_split", None)
        if split not in ("train", "val", "test"):
            return

        #2) acclerator ë° main process ì—¬ë¶€
        acc = getattr(self, "accelerator", None)
        is_main = (acc.is_main_process if acc is not None else True) #is_main=True â†’ë¡œê·¸ ì¶œë ¥ ë° ìµœì¢… metric ì €ì¥ì€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì§„í–‰.
        device = ctx.device

        #3) ìƒ˜í”Œ ìˆ˜ì™€ loss ì§‘ê³„. ê·¸ë¦¬ê³  ì§‘ê³„ëœê²ƒ ë°”íƒ•ìœ¼ë¡œ í‰ê·  loss êµ¬í•¨.
        n_global, loss_global = self._ddp_reduce_split(ctx, split)
        avg_global = float(loss_global / max(n_global, 1))

        #4) ì •í™•ë„ ê³„ì‚° ì¤€ë¹„. ìƒ˜í”Œ/ì •ë‹µ ì§‘ê³„ í…ì„œí™”
        t_seen = torch.tensor(
            [int(getattr(ctx, "sample_seen", 0))],
            device=device,
            dtype=torch.long,
        )
        t_corr = torch.tensor(
            [int(getattr(ctx, "sample_correct_accum", 0))],
            device=device,
            dtype=torch.long,
        )

        # ë¶„ì‚°ì´ë©´ í•©ì‚°
        if acc is not None and acc.num_processes > 1:
            t_seen = acc.reduce(t_seen, reduction="sum")
            t_corr = acc.reduce(t_corr, reduction="sum")

        seen_global = int(t_seen.item())
        corr_global = int(t_corr.item())
        acc_global = float(corr_global / max(seen_global, 1)) if seen_global > 0 else 0.0

        if is_main:
            if not hasattr(ctx, "eval_metrics") or ctx.eval_metrics is None:
                ctx.eval_metrics = {}

            ctx.eval_metrics.update({
                f"{split}_total":    n_global,
                f"{split}_loss":     loss_global,
                f"{split}_avg_loss": avg_global,
                f"{split}_acc":      acc_global,
            })

            logger.info(
                f"[{split}|reduce] total={n_global}, "
                f"loss_sum={loss_global:.6f}, avg_loss={avg_global:.6f}, "
                f"acc={acc_global:.6f} "
                f"(local_seen={getattr(ctx,'sample_seen',0)}, "
                f"local_corr={getattr(ctx,'sample_correct_accum',0)})"
            )
        else:
            ctx.eval_metrics = {}



def call_reward_choice_trainer(trainer_type):
    if trainer_type == 'llmrewardchoicetrainer': return RewardChoiceTrainer

register_trainer('llmrewardchoicetrainer', call_reward_choice_trainer)
